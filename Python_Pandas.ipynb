{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python for Spatial Analysis\n",
    "## Second part of the module of GG3209 Spatial Analysis with GIS.\n",
    "### Notebook to learn and practice Pandas\n",
    "\n",
    "---\n",
    "Dr Fernando Benitez -  University of St Andrews - School of Geography and Sustainable Development - First Iteration 2023 v.1.0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction \n",
    "\n",
    "After practicing **NumPy**, this notebook aims to work with the library **Pandas** which allows for reading in and working with data tables.\n",
    "\n",
    "Most geo spatial scientists are first introduced to data tables in the form of an Excel Spreadsheet. In such a structure, each record or feature is represented by a row of data while each column represents a specific piece of information for each record. Sometimes we call that a variable, or attribute. \n",
    "\n",
    "Further, spreadsheets are able to hold different data types in each column. A comparable data structure would be handy for use in Python. This is made available by the **Pandas** library. Pandas allows for data to be stored in **DataFrames**. If you work in the R environment, this is very similar to the concept of data frames in R. In fact, *Pandas DataFrames were inspired by R data frames*. \n",
    "\n",
    "Pandas makes use of the NumPy library, so it is generally a good idea to import NumPy if you plan to use Pandas. Also, you will need to install Pandas and NumPy into your environment prior to using them., but we have include both libraries in our python environment py4sa.yml\n",
    "\n",
    "The complete documentation for Pandas can be found [here](https://pandas.pydata.org/).\n",
    "\n",
    "After working through this module you will be able to:\n",
    "\n",
    "1. Create and manipulate **Series** and **DataFrames** using Pandas.\n",
    "2. Query and subset DataFrames.\n",
    "3. Manipulate DataFrames.\n",
    "4. Summarize and group DataFrames.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro to Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we talk about DataFrames, I will introduce the concept of a **Series**. These are actually very similar to a NumPy array except that they allow for axis labels to be assigned. Examples of generating a series from **lists**, **NumPy arrays**, and **dictionaries** are provided below. \n",
    "\n",
    "A series is comparable to a single column from a Spreadsheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class1                    GIS\n",
      "Class2         Remote Sensing\n",
      "Class3       Spatial Analysis\n",
      "Class4    Digital Cartography\n",
      "dtype: object\n",
      "Class1    350\n",
      "Class2    455\n",
      "Class3    457\n",
      "Class4    642\n",
      "dtype: int64\n",
      "Class1                    GIS\n",
      "Class2         Remote Sensing\n",
      "Class3       Spatial Analysis\n",
      "Class4    Digital Cartography\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "lst1 = [\"GIS\", \"Remote Sensing\", \"Spatial Analysis\", \"Digital Cartography\"]\n",
    "arr1 = np.array([350, 455, 457, 642])\n",
    "dict1 = {'Class1':\"GIS\", \"Class2\":\"Remote Sensing\", \"Class3\":\"Spatial Analysis\", \"Class4\":\"Digital Cartography\"}\n",
    "\n",
    "s_lst = pd.Series(data=lst1, index = [\"Class1\", \"Class2\", \"Class3\", \"Class4\"])\n",
    "s_arr = pd.Series(data=arr1, index = [\"Class1\", \"Class2\", \"Class3\", \"Class4\"])\n",
    "s_dict = pd.Series(dict1)\n",
    "\n",
    "print(s_lst)\n",
    "print(s_arr)\n",
    "print(s_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels or names can then be used to select data either using **bracket notation** or **dot notation**. \n",
    "\n",
    "You can use whichever method you prefer. However, if you use dot notation you should not included spaces in the column names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spatial Analysis\n",
      "Spatial Analysis\n"
     ]
    }
   ],
   "source": [
    "print(s_dict[\"Class3\"])\n",
    "print(s_dict.Class3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by building a DataFrame from a set of lists. First, you can create three lists to hold different components of course title information.\n",
    "\n",
    "Next, you could combine these lists into a dictionary. Finally, you can convert the dictionary into a DataFrame. \n",
    "\n",
    "Note that a well formatted table is generated by just calling the DataFrame name without the *print()* function; however, in the example we use *print()* (but try out both ways to see the difference). Also, the **keys** from the dictionary have been used as the column names, and a default index has been assigned to each row.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  prefix  course_number          course_name\n",
      "0   Geol            103   Earth Through Time\n",
      "1   Geol            321        Geomorphology\n",
      "2   Geol            331         Paleontology\n",
      "3   Geol            341   Structural Geology\n",
      "4   Geog            350            GIScience\n",
      "5   Geog            455       Remote Sensing\n",
      "6   Geog            462  Digital Cartography\n"
     ]
    }
   ],
   "source": [
    "prefix = [\"Geol\", \"Geol\", \"Geol\", \"Geol\", \"Geog\", \"Geog\", \"Geog\"]\n",
    "cnum = [103, 321, 331, 341, 350, 455, 462]\n",
    "cname = [\"Earth Through Time\", \"Geomorphology\", \"Paleontology\", \"Structural Geology\", \"GIScience\", \"Remote Sensing\", \"Digital Cartography\"]\n",
    "course_dict = {\"prefix\": prefix, \"course_number\": cnum, \"course_name\": cname}\n",
    "course_df = pd.DataFrame(course_dict)\n",
    "#course_df\n",
    "print(course_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since column names are assigned, they can be used to select out individual columns using bracket or dot notation. Single columns can be saved as a Series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     Earth Through Time\n",
      "1          Geomorphology\n",
      "2           Paleontology\n",
      "3     Structural Geology\n",
      "4              GIScience\n",
      "5         Remote Sensing\n",
      "6    Digital Cartography\n",
      "Name: course_name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(course_df[\"course_name\"])\n",
    "# Or course_df.course_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A list of column names can be provided to subset out multiple columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   course_number          course_name\n",
      "0            103   Earth Through Time\n",
      "1            321        Geomorphology\n",
      "2            331         Paleontology\n",
      "3            341   Structural Geology\n",
      "4            350            GIScience\n",
      "5            455       Remote Sensing\n",
      "6            462  Digital Cartography\n"
     ]
    }
   ],
   "source": [
    "print(course_df[[\"course_number\", \"course_name\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing with ```.loc``` and ```.iloc```\n",
    "\n",
    "Pandas created the methods **.loc[]** and **.iloc[]** as more flexible alternatives for accessing data from a dataframe. \n",
    "\n",
    "Use ```df.iloc[]``` for indexing with integers and ```df.loc[]``` for indexing with labels.\n",
    "\n",
    "These are typically the recommended methods of indexing in Pandas\n",
    "\n",
    "1. Using the **.loc** method, you can subset based on **column names and row labels** combined, and \n",
    "\n",
    "2. The **.iloc** method, in contrast, is used for **selection based on indexes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   course_number    course_name\n",
      "1            321  Geomorphology\n",
      "2            331   Paleontology\n",
      "4            350      GIScience\n"
     ]
    }
   ],
   "source": [
    "print(course_df.loc[[1, 2, 4],[\"course_number\", \"course_name\"]])\n",
    "# Or course_df.iloc[[1, 2, 4],[1, 2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can even use the data stored in existing columns to create a new column. Note that the new column does not need to be declared prior to writing to it. In the example, I have written the entire course name to a new column. The *map()* method is used to make sure all data are treated as strings. It allow for the same function, in this case **str()**, to be applied to each element in an iterable, in this case each row in the DataFrame. I am including blank spaces so that the components are not ran together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  prefix  course_number          course_name                      full_name\n",
      "0   Geol            103   Earth Through Time   Geol 103: Earth Through Time\n",
      "1   Geol            321        Geomorphology        Geol 321: Geomorphology\n",
      "2   Geol            331         Paleontology         Geol 331: Paleontology\n",
      "3   Geol            341   Structural Geology   Geol 341: Structural Geology\n",
      "4   Geog            350            GIScience            Geog 350: GIScience\n",
      "5   Geog            455       Remote Sensing       Geog 455: Remote Sensing\n",
      "6   Geog            462  Digital Cartography  Geog 462: Digital Cartography\n"
     ]
    }
   ],
   "source": [
    "course_df[\"full_name\"] = course_df[\"prefix\"].map(str)  + \" \" + course_df[\"course_number\"].map(str)  + \": \" + course_df[\"course_name\"].map(str) \n",
    "print(course_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading external files\n",
    "\n",
    "Instead of creating data tables or DataFrames manually, you are probably more likely to read in a data table from a file (CSV) or web link. \n",
    "\n",
    "Fortunately, **Pandas** provides functions for reading data in from a variety of formats. Here are some examples:\n",
    "\n",
    "* *read_table()*: delimited file (TXT, CSV, etc.)\n",
    "* *read_csv()*: comma-separated values (CSV)\n",
    "* *read_excel()*: Excel Spreadsheet\n",
    "* *read_json()*: JavaScript Object Notation (JSON)\n",
    "* *read_html()*: HTML table\n",
    "* *read_sas()*: SAS file\n",
    "\n",
    "Full documentation on reading in data can be found [here](https://pandas.pydata.org/docs/reference/io.html).\n",
    "\n",
    "In the example below, I am reading in a CSV file from my local computer. The *sep* argument is used to define the deliminator ( like you have done in Excel).\n",
    "\n",
    "However, commas are the default, so *it isn't necessary to include this argument in this case*.\n",
    "\n",
    "Setting the *header* argument to 0 indicated that the first row of data should be treated as column names or headers. \n",
    "\n",
    "It isn't always necessary to specify the character encoding; But a best practice tells that it is necessary due to the use of special characters in some tables. \n",
    "\n",
    "To view the first 10 rows of the data. You can use the *head()* method.\n",
    "\n",
    "The *len()* function returns the number of rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>country</th>\n",
       "      <th>pop</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>capital</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'Abasan al-Jadidah</td>\n",
       "      <td>Palestine</td>\n",
       "      <td>5629</td>\n",
       "      <td>31.31</td>\n",
       "      <td>34.34</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'Abasan al-Kabirah</td>\n",
       "      <td>Palestine</td>\n",
       "      <td>18999</td>\n",
       "      <td>31.32</td>\n",
       "      <td>34.35</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'Abdul Hakim</td>\n",
       "      <td>Pakistan</td>\n",
       "      <td>47788</td>\n",
       "      <td>30.55</td>\n",
       "      <td>72.11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'Abdullah-as-Salam</td>\n",
       "      <td>Kuwait</td>\n",
       "      <td>21817</td>\n",
       "      <td>29.36</td>\n",
       "      <td>47.98</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'Abud</td>\n",
       "      <td>Palestine</td>\n",
       "      <td>2456</td>\n",
       "      <td>32.03</td>\n",
       "      <td>35.07</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>'Abwein</td>\n",
       "      <td>Palestine</td>\n",
       "      <td>3434</td>\n",
       "      <td>32.03</td>\n",
       "      <td>35.20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>'Adadlay</td>\n",
       "      <td>Somalia</td>\n",
       "      <td>9198</td>\n",
       "      <td>9.77</td>\n",
       "      <td>44.65</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>'Adale</td>\n",
       "      <td>Somalia</td>\n",
       "      <td>5492</td>\n",
       "      <td>2.75</td>\n",
       "      <td>46.30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>'Afak</td>\n",
       "      <td>Iraq</td>\n",
       "      <td>22706</td>\n",
       "      <td>32.07</td>\n",
       "      <td>45.26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>'Afif</td>\n",
       "      <td>Saudi Arabia</td>\n",
       "      <td>41731</td>\n",
       "      <td>23.92</td>\n",
       "      <td>42.93</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 city       country    pop    lat    lon  capital\n",
       "0  'Abasan al-Jadidah     Palestine   5629  31.31  34.34        0\n",
       "1  'Abasan al-Kabirah     Palestine  18999  31.32  34.35        0\n",
       "2        'Abdul Hakim      Pakistan  47788  30.55  72.11        0\n",
       "3  'Abdullah-as-Salam        Kuwait  21817  29.36  47.98        0\n",
       "4               'Abud     Palestine   2456  32.03  35.07        0\n",
       "5             'Abwein     Palestine   3434  32.03  35.20        0\n",
       "6            'Adadlay       Somalia   9198   9.77  44.65        0\n",
       "7              'Adale       Somalia   5492   2.75  46.30        0\n",
       "8               'Afak          Iraq  22706  32.07  45.26        0\n",
       "9               'Afif  Saudi Arabia  41731  23.92  42.93        0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cities_df = pd.read_csv(\"data/world_cities.csv\", sep=\",\", header=0, encoding=\"ISO-8859-1\")\n",
    "cities_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43645\n"
     ]
    }
   ],
   "source": [
    "print(len(cities_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame Query and Subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now use this data table to explore data query and selection methods. \n",
    "\n",
    "In the first example, I am selecting out Italian cities and saving them to a new DataFrame. Note the use of bracket notation. The code in the middle bracket is used to perform the selection. \n",
    "\n",
    "The second example includes a compound query. Note the use of parenthesis within the query. \n",
    "\n",
    "Lastly, it is also possible to subset out only certain columns that meet the query. In the last example, I am subsetting out just the name of the city and population. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      city country    pop    lat    lon  capital\n",
      "181            Abano Terme   Italy  19167  45.35  11.78        0\n",
      "197          Abbiategrasso   Italy  30515  45.41   8.91        0\n",
      "310                 Acerra   Italy  51394  41.00  14.39        0\n",
      "329           Aci Castello   Italy  17930  37.56  15.13        0\n",
      "330             Aci Catena   Italy  28459  37.60  15.14        0\n",
      "331       Aci Sant'Antonio   Italy  17566  37.60  15.11        0\n",
      "335               Acireale   Italy  53476  37.62  15.17        0\n",
      "344  Acquaviva delle Fonti   Italy  21555  40.89  16.84        0\n",
      "345            Acqui Terme   Italy  20634  44.68   8.46        0\n",
      "346                   Acri   Italy  21484  39.49  16.38        0\n",
      "\n",
      "985\n",
      "\n",
      "          city country      pop    lat    lon  capital\n",
      "12392    Genoa   Italy   599064  44.42   8.93        0\n",
      "23820    Milan   Italy  1316218  45.48   9.19        0\n",
      "25403   Naples   Italy   983614  40.85  14.27        0\n",
      "27926  Palermo   Italy   668275  38.12  13.36        0\n",
      "31558     Rome   Italy  2561181  41.89  12.50        1\n",
      "39214    Turin   Italy   873123  45.08   7.68        0\n",
      "\n",
      "6\n",
      "\n",
      "         city      pop\n",
      "12392   Genoa   599064\n",
      "23820   Milan  1316218\n",
      "25403  Naples   983614\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Example 1\n",
    "just_italy = cities_df[cities_df[\"country\"]==\"Italy\"]\n",
    "print(just_italy.head(10))\n",
    "print('')\n",
    "print(len(just_italy))\n",
    "print('')\n",
    "#Example 2\n",
    "italy_med_cities = cities_df[(cities_df[\"country\"]==\"Italy\") & (cities_df[\"pop\"]>500000)]\n",
    "print(italy_med_cities)\n",
    "print('')\n",
    "print(len(italy_med_cities))\n",
    "print('')\n",
    "#Example 3\n",
    "italy_cities_pop = cities_df[(cities_df[\"country\"]==\"Italy\") & (cities_df[\"pop\"]>500000)][[\"city\", \"pop\"]]\n",
    "print(italy_cities_pop.head(3))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option for performing queries is to use the **query()** method provided by Pandas. When using this method, the query will need to be provided as an expression in string form. Also, spaces in column names can be problematic, so spaces should be removed or replaced with underscores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove spaces in column names using list comprehension\n",
    "cities_df.columns = [column.replace(\" \", \"_\") for column in cities_df.columns]\n",
    "#Example 1\n",
    "just_spain = cities_df.query('country==\"Spain\"')\n",
    "print(just_spain.head(10))\n",
    "print('')\n",
    "print(len(just_spain))\n",
    "print('')\n",
    "#Example 2\n",
    "spanish_cities_df = cities_df.query('country==\"Spain\" and pop > 500000')\n",
    "print(len(spanish_cities_df))\n",
    "print('')\n",
    "#Example 3\n",
    "subset_query = cities_df.query('country==\"Spain\" and pop > 500000')[[\"city\", \"pop\"]]\n",
    "print(subset_query)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a query is complete, you may want to save the result back to a file on your local machine. The code below provides an example for saving out the last subset of data to a CSV file. The Pandas documentation provides examples for saving to other formats. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_query.to_csv(\"data/subset_data.csv\", sep=\",\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *NULL*, *NoData*, or missing indicator in Python is *NaN*. To begin exploring missing values, let's recode some of the data to *NaN* in the cities data set. In the example below, I am changing the \"Germany\" and \"Palestine\" countries to *NaN*. I am also recoding any population between 10.000 and 50.000 to *NaN*. The *replace()* method is used to change the categories while the *mask()* method is used to recode the rating values. *np.nan* is a NumPy method for defining null values.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_nan = cities_df.copy()\n",
    "cities_nan[\"country\"] = cities_nan[[\"country\"]].replace([\"Germany\", \"Palestine\"], np.nan)\n",
    "cities_nan['pop'].mask(cities_nan['pop'].between(10000, 50000), inplace=True)\n",
    "print(cities_nan.head(10))\n",
    "print('')\n",
    "print(len(cities_nan))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *dropna()* method can be used to remove rows or columns that contain missing data. If the axis parameter is set to 0, rows with missing values in any column will be removed. If it is set to 1, columns with missing data in any row will be removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_drop = cities_nan.dropna(axis=0)\n",
    "print(cities_drop.head(10))\n",
    "print('')\n",
    "print(len(cities_drop))\n",
    "print('')\n",
    "cities_dropc = cities_nan.dropna(axis=1)\n",
    "print(cities_dropc.head(10))\n",
    "print('')\n",
    "print(len(cities_dropc))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *.fillna()* method can be used to replace NA values with another value or string. In the example, I am changing the missing genres to \"Unknown Country\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_nan[\"country\"] = cities_nan[\"country\"].fillna(value=\"Unknown Country\")\n",
    "print(cities_nan.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to replace null values with a statistic derived from the available values. In the example, I am replacing the missing population values with the mean of all available in the attribute. Of course this is not possible, here is just an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_nan[\"pop\"] = cities_nan[\"pop\"].fillna(value=cities_nan[\"pop\"].mean())\n",
    "print(cities_nan.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping and Summarizing DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas provides methods for summarizing data as described in the examples below. First, I am creating individual statistics and saving them to variables. I then create a Series from a dictionary of these statistics, convert it to a DataFrame using the *to_frame()* method, then transpose the DataFrame using *transpose()*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquake_df = pd.read_csv(\"data/Latest_earthquake_world.csv\", sep=\",\", header=0, encoding=\"ISO-8859-1\")\n",
    "earthquake_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquake_df = pd.read_csv(\"data/Latest_earthquake_world.csv\", sep=\",\", header=0, encoding=\"ISO-8859-1\")\n",
    "earthquake_df.columns = [column.replace(\" \", \"_\") for column in earthquake_df.columns]\n",
    "earth_cnt = earthquake_df[\"mag\"].count()\n",
    "earth_mn = earthquake_df[\"mag\"].mean()\n",
    "earth_max = earthquake_df[\"mag\"].max()\n",
    "earth_min = earthquake_df[\"mag\"].min()\n",
    "earth_rang = earth_max-earth_min\n",
    "earth_stats= pd.Series({\"Count\": earth_cnt, \"Mean\": earth_mn, \"Max\": earth_max, \"Min\": earth_min, \"Range\": earth_rang}).to_frame().transpose()\n",
    "print(earth_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to obtain summary statistics for each group separately by applying the very useful *group_by()* method. In the example below, I am obtaining stats for each MagSource and saving them into a DataFrame. The columns do not need to be defined beforehand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquake_df = pd.read_csv(\"data/Latest_earthquake_world.csv\", sep=\",\", header=0, encoding=\"ISO-8859-1\")\n",
    "earthquake_df.columns = [column.replace(\" \", \"_\") for column in earthquake_df.columns]\n",
    "earthquake_stats = pd.DataFrame()\n",
    "earthquake_stats[\"Count\"] = earthquake_df.groupby(\"magSource\")['mag'].count()\n",
    "earthquake_stats[\"Mean\"] = earthquake_df.groupby(\"magSource\")['mag'].mean()\n",
    "earthquake_stats[\"Max\"] = earthquake_df.groupby(\"magSource\")['mag'].max()\n",
    "earthquake_stats[\"Min\"] = earthquake_df.groupby(\"magSource\")['mag'].min()\n",
    "earthquake_stats[\"Range\"] = earthquake_stats[\"Max\"] - earthquake_stats[\"Min\"]\n",
    "earthquake_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *describe()* method can be used to obtain a set of default summary statistics for a column of data. Combining this with *group_by()* allows for the calculation of statistics by group. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(earthquake_df.groupby(\"mag\")[\"depth\"].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate and Merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pandas *concat()* method is used to **concatenate** DataFrames that have the same columns. This is comparable to copying and pasting rows from two spreadsheets into a new spreadsheet. To demonstrate this, I have extracted rows using indexes. Next, I concatenate them back to a new DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquake_df = pd.read_csv(\"data/Latest_earthquake_world.csv\", sep=\",\", header=0, encoding=\"ISO-8859-1\")\n",
    "earthquake_df.columns =[column.replace(\" \", \"_\") for column in earthquake_df.columns]\n",
    "\n",
    "earth_sub1 = earthquake_df[100:500]\n",
    "earth_sub2 = earthquake_df[900:1300]\n",
    "earth_subc = pd.concat([earth_sub1, earth_sub2])\n",
    "print(len(earthquake_df))\n",
    "print(len(earth_subc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merge** is comparable to table joins when using SQL. This requires the use of **keys** and the declaration of a joining method, such as \"Left\", \"Right\", \"Inner\", or \"Outer\". \n",
    "\n",
    "In the example, I first create a unique ID by copying the row index to a column (idx).\n",
    "\n",
    "I then break the data into two components, each containing the ID and a subset of the remaining columns. \n",
    "\n",
    "I then use the *merge()* method to merge the DataFrames using the \"inner\" method and the common \"id\" field. \"Inner\" will only return rows that occur in both data sets. Since both DataFrames were derived from the same original DataFrame, they will have identical rows, so the result would be the same as using \"left\", where all rows from the left table are maintained even if they don't occur in the right table, or \"right\", where all rows from the right table are maintained even if they don't occur in the left table. \n",
    "\n",
    "In the second example, I use a query to separate out only earthquakes with magnitude of more than 4.0. When I perform a join with all of the data using the \"inner\" method, I only get back the common or shared rows. \n",
    "\n",
    "Note that there is also a *join()* method that joins based on indexes. However, that will not be demonstrated here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m earthquake_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/Latest_earthquake_world.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mISO-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m earthquake_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midx\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m earthquake_df\u001b[38;5;241m.\u001b[39mindex\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(earthquake_df\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m2\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "earthquake_df = pd.read_csv(\"data/Latest_earthquake_world.csv\", sep=\",\", header=0, encoding=\"ISO-8859-1\")\n",
    "\n",
    "earthquake_df[\"idx\"] = earthquake_df.index\n",
    "print(earthquake_df.head(2))\n",
    "print('')\n",
    "print(len(earthquake_df))\n",
    "print('')\n",
    "subset_first = earthquake_df[[\"idx\", \"mag\"]]\n",
    "subset_second = earthquake_df[[\"idx\", \"depth\", \"place\"]]\n",
    "\n",
    "earth_merge = pd.merge(subset_first, subset_second, how=\"inner\", on=\"idx\")\n",
    "print(earth_merge.head(5))\n",
    "print('')\n",
    "print(len(earth_merge))\n",
    "print('')\n",
    "subset_third = earthquake_df.query('mag > 4')[[\"idx\", \"place\", \"depth\"]]\n",
    "earth_merge2 = pd.merge(subset_first, subset_third, how=\"inner\", on=\"idx\")\n",
    "print(earth_merge2.head(5))\n",
    "print('')\n",
    "print(len(earth_merge2))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well there is much to discuss and learn from the use of Pandas, but this is the initial step to get you familiarize with this library. Now it is your turn to try out the exercises to help you to recall and apply all the methods in these two notebook. So please open the **Exercises_NumPy_Pandas.ipynb** work on it. \n",
    "\n",
    "For more examples and details, please consult the documentation for [Pandas](https://pandas.pydata.org/docs/reference/io.html). \n",
    "\n",
    "In the next week, we will discuss methods for graphing and visualizing data using **matplotlib**, and **Pandas**. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "a62a218f45948969006c944db2ca1c519af623da5e08f864ae6aafcacb945df1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
