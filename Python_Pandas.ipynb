{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python for Spatial Analysis\n",
    "## Second part of the module of GG3209 Spatial Analysis with GIS.\n",
    "### Notebook to learn and practice Pandas\n",
    "\n",
    "---\n",
    "Dr Fernando Benitez -  University of St Andrews - School of Geography and Sustainable Development - First Iteration 2023 v.1.0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction \n",
    "\n",
    "After practicing **NumPy**, this notebook aims to work with the library **Pandas** which allows for reading in and working with data tables.\n",
    "\n",
    "Most geo spatial scientists are first introduced to data tables in the form of an Excel Spreadsheet. In such a structure, each record or feature is represented by a row of data while each column represents a specific piece of information for each record. Sometimes we call that a variable, or attribute. \n",
    "\n",
    "Further, spreadsheets are able to hold different data types in each column. A comparable data structure would be handy for use in Python. This is made available by the **Pandas** library. Pandas allows for data to be stored in **DataFrames**. If you work in the R environment, this is very similar to the concept of data frames in R. In fact, *Pandas DataFrames were inspired by R data frames*. \n",
    "\n",
    "Pandas makes use of the NumPy library, so it is generally a good idea to import NumPy if you plan to use Pandas. Also, you will need to install Pandas and NumPy into your environment prior to using them., but we have include both libraries in our python environment py4sa.yml\n",
    "\n",
    "The complete documentation for Pandas can be found [here](https://pandas.pydata.org/).\n",
    "\n",
    "After working through this module you will be able to:\n",
    "\n",
    "1. Create and manipulate **Series** and **DataFrames** using Pandas.\n",
    "2. Query and subset DataFrames.\n",
    "3. Manipulate DataFrames.\n",
    "4. Summarize and group DataFrames.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro to Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we talk about DataFrames, I will introduce the concept of a **Series**. These are actually very similar to a NumPy array except that they allow for axis labels to be assigned. Examples of generating a series from **lists**, **NumPy arrays**, and **dictionaries** are provided below. \n",
    "\n",
    "A series is comparable to a single column from a Spreadsheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst1 = [\"GIS\", \"Remote Sensing\", \"Spatial Analysis\", \"Digital Cartography\"]\n",
    "arr1 = np.array([350, 455, 457, 642])\n",
    "dict1 = {'Class1':\"GIS\", \"Class2\":\"Remote Sensing\", \"Class3\":\"Spatial Analysis\", \"Class4\":\"Digital Cartography\"}\n",
    "\n",
    "s_lst = pd.Series(data=lst1, index = [\"Class1\", \"Class2\", \"Class3\", \"Class4\"])\n",
    "s_arr = pd.Series(data=arr1, index = [\"Class1\", \"Class2\", \"Class3\", \"Class4\"])\n",
    "s_dict = pd.Series(dict1)\n",
    "\n",
    "print(s_lst)\n",
    "print(s_arr)\n",
    "print(s_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels or names can then be used to select data either using **bracket notation** or **dot notation**. \n",
    "\n",
    "You can use whichever method you prefer. However, if you use dot notation you should not included spaces in the column names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(s_dict[\"Class3\"])\n",
    "print(s_dict.Class3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by building a DataFrame from a set of lists. First, you can create three lists to hold different components of course title information.\n",
    "\n",
    "Next, you could combine these lists into a dictionary. Finally, you can convert the dictionary into a DataFrame. \n",
    "\n",
    "Note that a well formatted table is generated by just calling the DataFrame name without the *print()* function; however, in the example we use *print()* (but try out both ways to see the difference). Also, the **keys** from the dictionary have been used as the column names, and a default index has been assigned to each row.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = [\"Geol\", \"Geol\", \"Geol\", \"Geol\", \"Geog\", \"Geog\", \"Geog\"]\n",
    "cnum = [103, 321, 331, 341, 350, 455, 462]\n",
    "cname = [\"Earth Through Time\", \"Geomorphology\", \"Paleontology\", \"Structural Geology\", \"GIScience\", \"Remote Sensing\", \"Digital Cartography\"]\n",
    "course_dict = {\"prefix\": prefix, \"course_number\": cnum, \"course_name\": cname}\n",
    "course_df = pd.DataFrame(course_dict)\n",
    "#course_df\n",
    "print(course_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since column names are assigned, they can be used to select out individual columns using bracket or dot notation. Single columns can be saved as a Series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(course_df[\"course_name\"])\n",
    "# Or course_df.course_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A list of column names can be provided to subset out multiple columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(course_df[[\"course_number\", \"course_name\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing with ```.loc``` and ```.iloc```\n",
    "\n",
    "Pandas created the methods **.loc[]** and **.iloc[]** as more flexible alternatives for accessing data from a dataframe. \n",
    "\n",
    "Use ```df.iloc[]``` for indexing with integers and ```df.loc[]``` for indexing with labels.\n",
    "\n",
    "These are typically the recommended methods of indexing in Pandas\n",
    "\n",
    "1. Using the **.loc** method, you can subset based on **column names and row labels** combined, and \n",
    "\n",
    "2. The **.iloc** method, in contrast, is used for **selection based on indexes**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(course_df.loc[[1, 2, 4],[\"course_number\", \"course_name\"]])\n",
    "# Or course_df.iloc[[1, 2, 4],[1, 2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can even use the data stored in existing columns to create a new column. Note that the new column does not need to be declared prior to writing to it. In the example, I have written the entire course name to a new column. The *map()* method is used to make sure all data are treated as strings. It allow for the same function, in this case **str()**, to be applied to each element in an iterable, in this case each row in the DataFrame. I am including blank spaces so that the components are not ran together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_df[\"full_name\"] = course_df[\"prefix\"].map(str)  + \" \" + course_df[\"course_number\"].map(str)  + \": \" + course_df[\"course_name\"].map(str) \n",
    "print(course_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading external files\n",
    "\n",
    "Instead of creating data tables or DataFrames manually, you are probably more likely to read in a data table from a file (CSV) or web link. \n",
    "\n",
    "Fortunately, **Pandas** provides functions for reading data in from a variety of formats. Here are some examples:\n",
    "\n",
    "* *read_table()*: delimited file (TXT, CSV, etc.)\n",
    "* *read_csv()*: comma-separated values (CSV)\n",
    "* *read_excel()*: Excel Spreadsheet\n",
    "* *read_json()*: JavaScript Object Notation (JSON)\n",
    "* *read_html()*: HTML table\n",
    "* *read_sas()*: SAS file\n",
    "\n",
    "Full documentation on reading in data can be found [here](https://pandas.pydata.org/docs/reference/io.html).\n",
    "\n",
    "In the example below, I am reading in a CSV file from my local computer. The *sep* argument is used to define the deliminator ( like you have done in Excel).\n",
    "\n",
    "However, commas are the default, so *it isn't necessary to include this argument in this case*.\n",
    "\n",
    "Setting the *header* argument to 0 indicated that the first row of data should be treated as column names or headers. \n",
    "\n",
    "It isn't always necessary to specify the character encoding; But a best practice tells that it is necessary due to the use of special characters in some tables. \n",
    "\n",
    "To view the first 10 rows of the data. You can use the *head()* method.\n",
    "\n",
    "The *len()* function returns the number of rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_df = pd.read_csv(\"data/world_cities.csv\", sep=\",\", header=0, encoding=\"ISO-8859-1\")\n",
    "cities_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(cities_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame Query and Subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now use this data table to explore data query and selection methods. \n",
    "\n",
    "In the first example, I am selecting out Italian cities and saving them to a new DataFrame. Note the use of bracket notation. The code in the middle bracket is used to perform the selection. \n",
    "\n",
    "The second example includes a compound query. Note the use of parenthesis within the query. \n",
    "\n",
    "Lastly, it is also possible to subset out only certain columns that meet the query. In the last example, I am subsetting out just the name of the city and population. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example 1\n",
    "just_italy = cities_df[cities_df[\"country\"]==\"Italy\"]\n",
    "print(just_italy.head(10))\n",
    "print('')\n",
    "print(len(just_italy))\n",
    "print('')\n",
    "#Example 2\n",
    "italy_med_cities = cities_df[(cities_df[\"country\"]==\"Italy\") & (cities_df[\"pop\"]>500000)]\n",
    "print(italy_med_cities)\n",
    "print('')\n",
    "print(len(italy_med_cities))\n",
    "print('')\n",
    "#Example 3\n",
    "italy_cities_pop = cities_df[(cities_df[\"country\"]==\"Italy\") & (cities_df[\"pop\"]>500000)][[\"city\", \"pop\"]]\n",
    "print(italy_cities_pop.head(3))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option for performing queries is to use the **query()** method provided by Pandas. When using this method, the query will need to be provided as an expression in string form. Also, spaces in column names can be problematic, so spaces should be removed or replaced with underscores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove spaces in column names using list comprehension\n",
    "cities_df.columns = [column.replace(\" \", \"_\") for column in cities_df.columns]\n",
    "#Example 1\n",
    "just_spain = cities_df.query('country==\"Spain\"')\n",
    "print(just_spain.head(10))\n",
    "print('')\n",
    "print(len(just_spain))\n",
    "print('')\n",
    "#Example 2\n",
    "spanish_cities_df = cities_df.query('country==\"Spain\" and pop > 500000')\n",
    "print(len(spanish_cities_df))\n",
    "print('')\n",
    "#Example 3\n",
    "subset_query = cities_df.query('country==\"Spain\" and pop > 500000')[[\"city\", \"pop\"]]\n",
    "print(subset_query)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a query is complete, you may want to save the result back to a file on your local machine. The code below provides an example for saving out the last subset of data to a CSV file. The Pandas documentation provides examples for saving to other formats. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_query.to_csv(\"data/subset_data.csv\", sep=\",\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *NULL*, *NoData*, or missing indicator in Python is *NaN*. To begin exploring missing values, let's recode some of the data to *NaN* in the cities data set. In the example below, I am changing the \"Germany\" and \"Palestine\" countries to *NaN*. I am also recoding any population between 10.000 and 50.000 to *NaN*. The *replace()* method is used to change the categories while the *mask()* method is used to recode the rating values. *np.nan* is a NumPy method for defining null values.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_nan = cities_df.copy()\n",
    "cities_nan[\"country\"] = cities_nan[[\"country\"]].replace([\"Germany\", \"Palestine\"], np.nan)\n",
    "cities_nan['pop'].mask(cities_nan['pop'].between(10000, 50000), inplace=True)\n",
    "print(cities_nan.head(10))\n",
    "print('')\n",
    "print(len(cities_nan))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *dropna()* method can be used to remove rows or columns that contain missing data. If the axis parameter is set to 0, rows with missing values in any column will be removed. If it is set to 1, columns with missing data in any row will be removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_drop = cities_nan.dropna(axis=0)\n",
    "print(cities_drop.head(10))\n",
    "print('')\n",
    "print(len(cities_drop))\n",
    "print('')\n",
    "cities_dropc = cities_nan.dropna(axis=1)\n",
    "print(cities_dropc.head(10))\n",
    "print('')\n",
    "print(len(cities_dropc))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *.fillna()* method can be used to replace NA values with another value or string. In the example, I am changing the missing genres to \"Unknown Country\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_nan[\"country\"] = cities_nan[\"country\"].fillna(value=\"Unknown Country\")\n",
    "print(cities_nan.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to replace null values with a statistic derived from the available values. In the example, I am replacing the missing population values with the mean of all available in the attribute. Of course this is not possible, here is just an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_nan[\"pop\"] = cities_nan[\"pop\"].fillna(value=cities_nan[\"pop\"].mean())\n",
    "print(cities_nan.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping and Summarizing DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas provides methods for summarizing data as described in the examples below. First, I am creating individual statistics and saving them to variables. I then create a Series from a dictionary of these statistics, convert it to a DataFrame using the *to_frame()* method, then transpose the DataFrame using *transpose()*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquake_df = pd.read_csv(\"data/Latest_earthquake_world.csv\", sep=\",\", header=0, encoding=\"ISO-8859-1\")\n",
    "earthquake_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquake_df = pd.read_csv(\"data/Latest_earthquake_world.csv\", sep=\",\", header=0, encoding=\"ISO-8859-1\")\n",
    "earthquake_df.columns = [column.replace(\" \", \"_\") for column in earthquake_df.columns]\n",
    "earth_cnt = earthquake_df[\"mag\"].count()\n",
    "earth_mn = earthquake_df[\"mag\"].mean()\n",
    "earth_max = earthquake_df[\"mag\"].max()\n",
    "earth_min = earthquake_df[\"mag\"].min()\n",
    "earth_rang = earth_max-earth_min\n",
    "earth_stats= pd.Series({\"Count\": earth_cnt, \"Mean\": earth_mn, \"Max\": earth_max, \"Min\": earth_min, \"Range\": earth_rang}).to_frame().transpose()\n",
    "print(earth_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to obtain summary statistics for each group separately by applying the very useful *group_by()* method. In the example below, I am obtaining stats for each MagSource and saving them into a DataFrame. The columns do not need to be defined beforehand. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquake_df = pd.read_csv(\"data/Latest_earthquake_world.csv\", sep=\",\", header=0, encoding=\"ISO-8859-1\")\n",
    "earthquake_df.columns = [column.replace(\" \", \"_\") for column in earthquake_df.columns]\n",
    "earthquake_stats = pd.DataFrame()\n",
    "earthquake_stats[\"Count\"] = earthquake_df.groupby(\"magSource\")['mag'].count()\n",
    "earthquake_stats[\"Mean\"] = earthquake_df.groupby(\"magSource\")['mag'].mean()\n",
    "earthquake_stats[\"Max\"] = earthquake_df.groupby(\"magSource\")['mag'].max()\n",
    "earthquake_stats[\"Min\"] = earthquake_df.groupby(\"magSource\")['mag'].min()\n",
    "earthquake_stats[\"Range\"] = earthquake_stats[\"Max\"] - earthquake_stats[\"Min\"]\n",
    "earthquake_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *describe()* method can be used to obtain a set of default summary statistics for a column of data. Combining this with *group_by()* allows for the calculation of statistics by group. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(earthquake_df.groupby(\"mag\")[\"depth\"].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate and Merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pandas *concat()* method is used to **concatenate** DataFrames that have the same columns. This is comparable to copying and pasting rows from two spreadsheets into a new spreadsheet. To demonstrate this, I have extracted rows using indexes. Next, I concatenate them back to a new DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquake_df = pd.read_csv(\"data/Latest_earthquake_world.csv\", sep=\",\", header=0, encoding=\"ISO-8859-1\")\n",
    "earthquake_df.columns =[column.replace(\" \", \"_\") for column in earthquake_df.columns]\n",
    "\n",
    "earth_sub1 = earthquake_df[100:500]\n",
    "earth_sub2 = earthquake_df[900:1300]\n",
    "earth_subc = pd.concat([earth_sub1, earth_sub2])\n",
    "print(len(earthquake_df))\n",
    "print(len(earth_subc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merge** is comparable to table joins when using SQL. This requires the use of **keys** and the declaration of a joining method, such as \"Left\", \"Right\", \"Inner\", or \"Outer\". \n",
    "\n",
    "In the example, I first create a unique ID by copying the row index to a column (idx).\n",
    "\n",
    "I then break the data into two components, each containing the ID and a subset of the remaining columns. \n",
    "\n",
    "I then use the *merge()* method to merge the DataFrames using the \"inner\" method and the common \"id\" field. \"Inner\" will only return rows that occur in both data sets. Since both DataFrames were derived from the same original DataFrame, they will have identical rows, so the result would be the same as using \"left\", where all rows from the left table are maintained even if they don't occur in the right table, or \"right\", where all rows from the right table are maintained even if they don't occur in the left table. \n",
    "\n",
    "In the second example, I use a query to separate out only earthquakes with magnitude of more than 4.0. When I perform a join with all of the data using the \"inner\" method, I only get back the common or shared rows. \n",
    "\n",
    "Note that there is also a *join()* method that joins based on indexes. However, that will not be demonstrated here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquake_df = pd.read_csv(\"data/Latest_earthquake_world.csv\", sep=\",\", header=0, encoding=\"ISO-8859-1\")\n",
    "\n",
    "earthquake_df[\"idx\"] = earthquake_df.index\n",
    "print(earthquake_df.head(2))\n",
    "print('')\n",
    "print(len(earthquake_df))\n",
    "print('')\n",
    "subset_first = earthquake_df[[\"idx\", \"mag\"]]\n",
    "subset_second = earthquake_df[[\"idx\", \"depth\", \"place\"]]\n",
    "\n",
    "earth_merge = pd.merge(subset_first, subset_second, how=\"inner\", on=\"idx\")\n",
    "print(earth_merge.head(5))\n",
    "print('')\n",
    "print(len(earth_merge))\n",
    "print('')\n",
    "subset_third = earthquake_df.query('mag > 4')[[\"idx\", \"place\", \"depth\"]]\n",
    "earth_merge2 = pd.merge(subset_first, subset_third, how=\"inner\", on=\"idx\")\n",
    "print(earth_merge2.head(5))\n",
    "print('')\n",
    "print(len(earth_merge2))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well there is much to discuss and learn from the use of Pandas, but this is the initial step to get you familiarize with this library. Now it is your turn to try out the exercises to help you to recall and apply all the methods in these two notebook. So please open the **Exercises_NumPy_Pandas.ipynb** work on it. \n",
    "\n",
    "For more examples and details, please consult the documentation for [Pandas](https://pandas.pydata.org/docs/reference/io.html). \n",
    "\n",
    "In the next week, we will discuss methods for graphing and visualizing data using **matplotlib**, and **Pandas**. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "a62a218f45948969006c944db2ca1c519af623da5e08f864ae6aafcacb945df1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
